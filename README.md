# ETL_project

    We performed ETL (extract, transform, load) on a set of csv files that were generated from a small clinical trial for patient response to a drug. The main point of this set of data is to determine if there is a difference between treatment groups (1mg/kg, 2mg/kg, placebo) between two ethnicities, caucasian and Japanese. Depending on the variation seen at different doses between ethnicities, it will determine how future clinical trials with this drug need to be designed (i.e. how much ethnic variation needs to be present in the trial). This is commonly used as gating criteria for future clinical trials and done on a smaller subset (less than 100 people). The actual clinical trial which will use the same analysis will pull data from thousands of patients. The following describes each of the steps we took to perform ETL on this dataset. 

    We extracted the sample data from a contract research organization's (CRO) database for each patient's cell population over time. The data contains information for lymphocytes (immune cells) and a couple lymphocyte subsets (T cells and macrophages). Each patient came had a sample drawn for 5 visits over the course of 15 days on drug or placebo. The analyzed data is a raw output for each patient ID containing visit day, cell type, cell count and percent of total (to name a few). A separate csv holds the patient information (i.e. treatment group, age, sex....etc.). This data was extracted from the company database.  

    We needed to transform the data through a series of steps in order to make it into a dataframe that would be loaded into a new database. This required the following steps. The data for the cell counts had records that failed the QC by the CRO. That can happen when the sample integrity is compromised either at blood draw or during shipment/preperation for analysis. These records needed to be dropped. We also needed to remove columns that were not necessary for our analysis. The patient information csv was a separate file. The reason for keeping these separate is to keep the initial data analysis by the CRO blinded to avoid expectation bias. However, to fully understand how treatment groups were impacted, these tables need to be merged and this was done on patient ID. We performed a JOIN with SQL after loading into that database as well as a merge in Pandas to show two ways of combining this data. 

    Because this was a relational dataset with all values defined and each relating back to a patient ID, we chose to use a relational database and uploaded our two tables (one from each csv file) to pgAdmin. We loaded the two tables directly from pandas code. 

