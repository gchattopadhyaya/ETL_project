# ETL_project
# Project Members: Lisa Nazer And Gargi Chattopadhyaya (We shared file locally in google drive and final git location was Lisas Github for the final push)


    We performed ETL (extract, transform, load) on a set of csv files that were generated from a small clinical trial for patient response to a drug. The main point of this set of data is to determine if there is a difference between treatment groups (1mg/kg, 2mg/kg, placebo) and any differences between two ethnicities, caucasian and Japanese. Depending on the variation seen at different doses between ethnicities, it will determine how future clinical trials with this drug need to be designed (i.e. how much ethnic variation needs to be present in the trial). This is commonly used as gating criteria for clinical trial design and done on a smaller number of patients than a normal trial (less than 100 people as compared to thousands). The following describes each of the steps we took to perform ETL on this dataset. 

    We extracted the sample data from a contract research organization's (CRO) database for each patient's cell population over time. The data contains information for lymphocytes (immune cells) and a couple lymphocyte subsets (T cells and macrophages). Each patient came had a sample drawn for 5 visits over the course of 15 days on drug or placebo. The analyzed data is a raw output for each patient ID containing visit day, cell type, cell count and percent of total (to name a few). A separate csv holds the patient information (i.e. treatment group, age, sex....etc.). This data was extracted from the company database.  

    We needed to transform the data through a series of steps in order to make it into a dataframe that would be loaded into a new database. This required the following steps. The data for the cell counts had records that failed the QC by the CRO. That can happen when the sample integrity is compromised either at sample collection or during shipment/preperation for analysis. These records needed to be dropped. We also needed to remove columns that were not necessary for our analysis and rename the columns for better clarity. The patient information csv was a separate file as stated above. The reason for storing this data separately is to keep the initial data analysis by the CRO blinded to avoid expectation bias. However, to fully understand how treatment groups were impacted, these tables need to be merged and this was done on patient ID. We performed a JOIN with SQL after loading into that database as well as a merge in Pandas to show two ways of storing and joining this data. 

    Due to the relational nature of this data set, with all values defined and each relating back to a patient ID, we chose to use a relational database and uploaded our two tables (one from each csv file) to pgAdmin. We loaded the two tables directly from pandas code. We also loaded the merged dataframe from pandas into the SQL database. Either method works for downstream analysis by the company. 
    
    One difficulty we ran into was learning that postgres doesn't take camel case when generating column headers. We had camel case column headers defined in Pandas and when we tried to make the tables in pgAdmin, we were able to generate tables with no error message while using camel case but there were errors when trying to populate these columns from Pandas because postgres has stored the coumn headers as all lowercase. Because postgres didn't error while generating the tables, this was a frustrating one to figure out! 

